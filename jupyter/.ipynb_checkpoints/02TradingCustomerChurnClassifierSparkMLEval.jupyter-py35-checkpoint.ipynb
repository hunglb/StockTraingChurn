{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "import pandas as pd\n",
    "import json\n",
    "import time, sys, os, shutil, glob, io, requests\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline, Model, PipelineModel\n",
    "from pyspark.sql import SQLContext\n",
    "import dsx_core_utils\n",
    "# define variables\n",
    "args = {\"threshold\": {\"min_value\": 0.3, \"mid_value\": 0.7, \"metric\": \"accuracyScore\"}, \"published\": \"false\", \"evaluator_type\": \"multiclass\", \"dataset\": \"/datasets/TradingCustomerSparkMLEval.csv\"}\n",
    "model_path = os.getenv(\"DSX_PROJECT_DIR\") + \"/models/Trading Churn Risk Classification SparkML/1/model\"\n",
    "# create spark context\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "# load the input data\n",
    "input_data = os.getenv(\"DSX_PROJECT_DIR\") + args.get(\"dataset\")\n",
    "dataframe = SQLContext(sc).read.csv(input_data , header=\"true\", inferSchema = \"true\")\n",
    "# load the model from disk \n",
    "model_rf = PipelineModel.load(model_path)\n",
    "# generate predictions\n",
    "predictions = model_rf.transform(dataframe)\n",
    "# Create Evalutation JSON\n",
    "evaluation = dict()\n",
    "evaluation[\"metrics\"] = dict()\n",
    "evaluation[\"modelName\"] = \"Trading Churn Risk Classification SparkML\"\n",
    "evaluation[\"startTime\"] = int(time.time())\n",
    "evaluation[\"modelVersion\"] = \"1\"\n",
    "threshold = {'min_value': 0.3, 'mid_value': 0.7, 'metric': 'accuracyScore'}\n",
    "# replace \"label\" below with the numeric representation of the label column that you defined while training the model\n",
    "labelCol = \"label\"\n",
    "# create evaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=labelCol)\n",
    "# compute evaluations\n",
    "evaluation[\"metrics\"][\"accuracyScore\"] = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "evaluation[\"metrics\"][\"f1Score\"] = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
    "evaluation[\"metrics\"][\"weightedPrecisionScore\"] = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "evaluation[\"metrics\"][\"weightedRecallScore\"] = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "evaluation[\"metrics\"][\"threshold\"] = threshold\n",
    "if(evaluation[\"metrics\"][threshold.get('metric','INVALID_METRIC')] >= threshold.get('mid_value', 0.70)):\n",
    "    evaluation[\"performance\"] = \"good\"\n",
    "elif(evaluation[\"metrics\"][threshold.get('metric','INVALID_METRIC')] <= threshold.get('min_value', 0.25)):\n",
    "    evaluation[\"performance\"] = \"poor\"\n",
    "else:\n",
    "    evaluation[\"performance\"] = \"fair\"\n",
    "evaluations_file_path = os.getenv(\"DSX_PROJECT_DIR\") + '/models/' + str(\"Trading Churn Risk Classification SparkML\") + '/' + str(\"1\") + '/evaluations.json'\n",
    "if(os.path.isfile(evaluations_file_path)):\n",
    "    current_evaluations = json.load(open(evaluations_file_path))\n",
    "else:\n",
    "    current_evaluations = []\n",
    "current_evaluations.append(evaluation)\n",
    "with open(evaluations_file_path, 'w') as outfile:\n",
    "    json.dump(current_evaluations, outfile, indent=4, sort_keys=True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python3.5 with DSX Spark 2.2.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
